#!/usr/bin/env python3
"""vLLM Docker Launcher

Manages vLLM container lifecycle:
- Reads config YAML
- Generates docker-compose.yaml from template
- Starts/stops container
- Waits for readiness
- Captures logs
"""
import argparse
import subprocess
import sys
import time
from pathlib import Path
from typing import Any

import yaml


def load_config(config_path: Path) -> dict[str, Any]:
    """Load configuration from YAML file."""
    with open(config_path) as f:
        return yaml.safe_load(f)


def generate_compose(config: dict[str, Any], output_path: Path) -> None:
    """Generate docker-compose.yaml from config."""
    # Build environment section - use KEY=VALUE format for docker compose
    env_vars = ["HUGGING_FACE_HUB_DISABLE_TELEMETRY=1", "HF_HOME=/root/.cache/huggingface"]
    if "env" in config:
        for key, value in config["env"].items():
            env_vars.append(f"{key}={value}")
    env_section = "\n      ".join(f"- {e}" for e in env_vars)

    # Build extra args section - ensure all values are quoted strings for YAML
    extra_args = config.get("extra_args", [])
    extra_args_section = "\n      ".join(f'- "{arg}"' for arg in extra_args)

    compose_content = f"""# Auto-generated compose file for {config['name']}
# DO NOT EDIT - generated by launcher.py

services:
  vllm:
    image: vllm/vllm-openai:v0.10.0
    container_name: vllm-server
    ports:
      - "8000:8000"
    ipc: host
    restart: "no"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      {env_section}
    command:
      - --model
      - {config['model_id']}
      - --served-model-name
      - {config['served_model_name']}
      - --host
      - "0.0.0.0"
      - --port
      - "8000"
      - --max-model-len
      - "{config['max_model_len']}"
      - --gpu-memory-utilization
      - "{config['gpu_memory_utilization']}"
      {extra_args_section}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
"""
    with open(output_path, "w") as f:
        f.write(compose_content)
    print(f"Generated compose file: {output_path}")


def stop_container() -> bool:
    """Stop and remove vLLM container."""
    print("Stopping vLLM container...")
    result = subprocess.run(
        ["docker", "compose", "-f", "compose.yaml", "down", "-v"],
        cwd=Path(__file__).parent,
        capture_output=True,
        text=True,
    )
    return result.returncode == 0


def start_container(compose_path: Path) -> subprocess.Popen:
    """Start vLLM container and return process."""
    print(f"Starting vLLM container from {compose_path}...")
    proc = subprocess.Popen(
        ["docker", "compose", "-f", str(compose_path), "up"],
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True,
    )
    return proc


def wait_for_ready(timeout_sec: int = 900, check_interval: int = 10) -> bool:
    """Wait for vLLM to be ready via /v1/models endpoint."""
    import urllib.request
    import urllib.error

    url = "http://127.0.0.1:8000/v1/models"
    start = time.time()
    print(f"Waiting for vLLM to be ready (timeout: {timeout_sec}s)...")

    while time.time() - start < timeout_sec:
        try:
            req = urllib.request.Request(url)
            with urllib.request.urlopen(req, timeout=5) as resp:
                if resp.status == 200:
                    print(f"vLLM ready after {time.time() - start:.1f}s")
                    return True
        except (urllib.error.URLError, TimeoutError, ConnectionRefusedError, OSError) as e:
            # Expected during startup - connection refused, timeout, etc.
            pass
        except Exception as e:
            # Unexpected error - log but continue waiting
            print(f"  Unexpected error checking readiness: {type(e).__name__}: {e}")
        time.sleep(check_interval)
        elapsed = time.time() - start
        print(f"  Still waiting... ({elapsed:.0f}s / {timeout_sec}s)")

    print(f"Timeout waiting for vLLM after {timeout_sec}s")
    return False


def capture_logs(output_path: Path) -> None:
    """Capture docker logs to file."""
    result = subprocess.run(
        ["docker", "logs", "vllm-server"],
        capture_output=True,
        text=True,
    )
    with open(output_path, "w") as f:
        f.write(result.stdout)
        if result.stderr:
            f.write("\n--- STDERR ---\n")
            f.write(result.stderr)
    print(f"Logs saved to {output_path}")


def main():
    parser = argparse.ArgumentParser(description="vLLM Docker Launcher")
    parser.add_argument("--config", type=Path, required=True, help="Config YAML file")
    parser.add_argument("--action", choices=["start", "stop", "generate"], required=True)
    parser.add_argument("--output-compose", type=Path, default=Path(__file__).parent / "compose.yaml")
    parser.add_argument("--log-output", type=Path, help="Path to save logs")
    parser.add_argument("--timeout", type=int, default=900, help="Readiness timeout in seconds")
    args = parser.parse_args()

    if args.action == "stop":
        stop_container()
        return 0

    config = load_config(args.config)
    print(f"Loaded config: {config['name']} - {config['description']}")

    if args.action == "generate":
        generate_compose(config, args.output_compose)
        return 0

    # Start action
    stop_container()  # Clean up any existing container
    generate_compose(config, args.output_compose)

    proc = start_container(args.output_compose)

    # Wait for ready in background while streaming logs
    ready = wait_for_ready(timeout_sec=args.timeout)

    if not ready:
        print("ERROR: vLLM failed to become ready")
        if args.log_output:
            capture_logs(args.log_output)
        stop_container()
        return 1

    if args.log_output:
        capture_logs(args.log_output)

    print(f"vLLM is running with config: {config['name']}")
    return 0


if __name__ == "__main__":
    sys.exit(main())

